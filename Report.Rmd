---
title: "Variable Selection Methods Comparison"
author: "Hongjie Liu, Jiajun Tao, Shaohan Chen"
header-includes:
   - \usepackage{graphicx}
   - \usepackage{float}
   - \usepackage{subfigure}
output:
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gridExtra)
library(tidyverse)
library(patchwork)
library(dplyr)
```


# Background and Objectives

## Background

In linear models, when facing high-dimensional data, variable selection method is a common practice to achieve a balance between model fitness and complexity. However, in modern high-dimensional data applications, traditional variable selection methods often struggle with the presence of "weak" predictors, i.e., predictors with small but non-zero coefficients.

## Objectives

This project aims to compare two popular variable selection methods, the step-wise forward method using the Akaike information criterion (AIC) and the automated LASSO regression. We conducted simulations under scenarios with high-dimensional data and larger numbers of observations to investigate how well each method performs in identifying weak and strong predictors and how missing weak predictors affects parameter estimations.

# Statistical Methods to be Studied

## Step-wise forward method

The step-wise forward method is an iterative process that starts with an empty model and sequentially adds variables that best improve the model fit, usually by adding predictors with the largest reduction in AIC. For linear models,
$$AIC = n\log\left(\sum_{i=1}^n (y_i - \widehat{y}_i)^2/n\right) + 2p,$$
where $\widehat{y}_i$ is the fitted values from a model.

## Automated LASSO regression

Automated LASSO regression estimates the model parameters by optimizing a penalized loss function:
$$\min_{\boldsymbol{\beta}} \frac{1}{2n} \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_i x_{ij} \right)^2 + \lambda \sum_{k=1}^p|\beta_k|,$$
where $\lambda\geq0$ is a tuning parameter. To select the optimal value of $\lambda$, we apply the 1SE (one-standard-error) rule, which involves computing the prediction error for each value of $\lambda$ and choosing the simplest model (i.e., the one with the fewest nonzero coefficients) whose prediction error is within one standard error of the minimum.

# Investigation Settings and Scenarios

## Definitions of Signal Types

This project aims to simulate data with a combination of strong, weak-but-correlated, weak-and-independent, and null predictors. The definitions of these predictor types are as follows.

Strong signals:
$$S_{strong}=\{j:|\beta_j|>c\sqrt{\log (p) / n}\ \mbox{for some } c>0,\  1\le j \le p\}$$
Weak-but-correlated (WBC) signals:
$$S_{WBC}=\ \{j: 0<|\beta_j|\le c\sqrt{\log (p) / n} \text{ and } \mbox{corr}(X_j, X_j')\ne 0\mbox{ for some } j'\in S_1,\  1\le j \le p\}$$
Weak-and-independent (WAI) signals:
$$S_{WAI}=\ \{j: 0<|\beta_j|\le c\sqrt{\log (p) / n} \text{ and } \mbox{corr}(X_j, X_j')= 0\mbox{ for all } j'\in S_1, \ 1\le j \le p\}$$
Null signals:
$$S_{null}=\{j: \beta_j=0,\  1\le j \le p\}$$

Thus, all $p$ signals can be partitioned as follows: $$\{1,\cdots,p\}=S_{strong}\cup S_{WBC}\cup S_{WAI}\cup S_{null}.$$
Let $p_{S}$, $p_{WBC}$, and $p_{WAI}$ denote the number of strong, WBC, and WAI predictors, respectively. The number of true predictors $p_{S}+p_{WBC}+p_{WAI}$ should not exceed the sample size $n$.

## Fixed Settings

In this project, we fix the number of the total predictors as 100, and set the ratio of true and null predictors as 2:3, which means there will be 40 true predictors and 60 null predictors. Meanwhile, we set the correlation between strong and weak-but-correlated signals as 0.4.

## Scenarios (Unfixed Settings)

In the following experiments, we modify some unfixed settings to see different model performances. We will change the number of observations from $n = 100$, to $n = 500$, $n = 2000$. When $n = 100$, the number of observations is the same as the number of predictors, which indicate it to be a high dimensional scenario. In addition, we will also try different ratios of strong, weak-but-correlated, and weak-and-independent signals as 1:4:5 and 3:3:4, in order to see if the ratio of different signals, especially the strong signals, will influence the model performances.

# Methods for Data Generation

## Data Generation - Response Vector

To generate the response vector $\mathbf{y}$, it is assumed that residuals are independent, normally distributed and have equal variances, so that $$\mathbf{y}\sim N(\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I}),$$
where $\mathbf{X}$ is the design matrix and $\boldsymbol{\beta}$ is the vector of coefficients.

For each $j\in S_{strong}$, the value of $\beta_j$ is set to 20, while for each $j\in S_{WBC}\cup S_{WAI}$, the value of $\beta_j$ is set to 0.5. We choose $c=20$ such that $0.5\leq c\sqrt{\log p/n} < 20$ for all scenarios to be investigated. The value of $\sigma$ for the error term is set to 8.


## Data Generation - Design Matrix

For each observations $\mathbf{x}_i$ in the design matrix $\mathbf{X}$, it is assumed that $$\mathbf{x}_i\overset{i.i.d.}{\sim} N(\boldsymbol{\mu},\boldsymbol{\Sigma}),\ i=1,\cdots,n,$$
where $\boldsymbol{\mu}$ is the mean vector and $\boldsymbol{\Sigma}$ is the covariance matrix.

To ensure fairness in the penalty of LASSO, it is assumed that all predictors are standardized, resulting in $\boldsymbol{\mu}=\mathbf{0}$ and $\Sigma_{ii}=1$ for all $i\in\{1,\cdots,n\}$, where $\Sigma_{ii}$ represents the $i$th diagonal element of the covariance matrix.

When all predictors are standardized, the covariance matrix $\boldsymbol{\Sigma}$ becomes a correlation matrix, allowing for the creation of correlations between WBC predictors and strong predictors. We set $p_{WBC}\geq p_{S}$. For each strong predictor (excluding one), $\left[\frac{p_{WBC}}{p_{S}}\right]$ WBC predictors are set to be correlated with it. Each WBC predictor is set to be correlated with only one strong predictor, with all other elements of $\boldsymbol{\Sigma}$ set to 0.

To generate data following a multivariate normal distribution, the **R**-function `mvrnorm` from the `MASS` package is used, with $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ passed as arguments.

# Model Evaluation

## Evaluation Metrics

In this project, we define the true predictors as positive and null predictors as negative.\par
For signal identification, we use the following five metrics to compare the two models:\par

- Complexity: number of selected predictors in the model

- Sensitivity: $\displaystyle\frac{TP}{TP+FN}$

- Specificity: $\displaystyle\frac{TN}{TN+FP}$

- F1-score: $\displaystyle\frac{2\cdot sensitivity\cdot precision}{sensitivity + precision}$

- Accuracy: $\displaystyle\frac{TP+TN}{TP+TN+FP+FN}$\par

For parameter estimation, we use the following to metrics to compare the two models:\par

- RMSE: $\displaystyle\sqrt{\frac{1}{p}\sum_{i=1}^p (\hat{\beta_i}-\beta_i)^2}$

- Variance: $\displaystyle\frac{1}{p}\sum_{i=1}^p (\hat{\beta_i}-\bar{\beta_i})^2$

## Signal Identification Performance

Complexity of the models is indicated by the number of selected predictors. We can see that in high dimensional scenario ($n=100$), forward selection model tends to select lots of predictors and Lasso tends to select very few. One interesting thing is that, if we increase the ratio of strong predictors (i.e., more strong predictors), Lasso also tends to select more predictors too. When it comes to normal scenario, forward selection still tends to select more predictors than Lasso, but the discrepancy is smaller than high dimensional case, and will be further narrowed down with $n$ or the ratio of strong predictors increasing. And as $n$ increases, the number of selected parameters of both models are closer to the true number 40.

As for overall classification performance, if in high dimensional scenario, forward selection tends to be very assertive and much better at identifying weak signals, leading to an extremely high sensitivity but low specificity. Lasso in turn tends to be very conservative and much better at identifying null signals, leading to an extremely high specificity but low sensitivity. Like high dimensional case, Lasso becomes more sensitive and not that assertive when the ratio of strong predictors increases. Based on the above plot, we can conclude that both models do not perform too well based on F1-score and accuracy, because they are very radical and tend to identify most of the predictors either as positive or negative, but far away from the truth.\par
For normal scenario, both models become less radical under normal scenarios, but forward selection is still more sensitive and assertive than Lasso, while Lasso has higher specificity and is more conservative. Both models perform better on those metrics with $n$ increasing. Overall, Lasso and forward selection has similar F1-score and accuracy performance. But when there are more strong predictors, Lasso performs obviously better than forward selection.

About the classification performance of different signals. Under all $n$ values, both models perfectly identify the strong signals. In high dimensional scenario, forward selection performs much better on identifying weak predictors while Lasso performs much better on identifying null predictors, that’s why we see the high sensitivity of forward selection and high specificity of Lasso in the previous section. When there are more strong predictors, Lasso also performs better on selecting weak predictors. When it comes to normal scenario, forward selection is still better at selecting null and Lasso is better at selecting weak predictors. But the discrepancy is smaller compared with high-dimensional data, and will continue to be smaller as $n$ increases. When there are more strong predictors, Lasso performs much better at selecting weak-but-correlated signals.

## Parameter Estimation Performance

In high dimensional scenario, Lasso performs much better than forward selection, with obviously much lower and centered RMSE and also lower variance. When it comes to normal scenario, when $n=500$, Lasso outperforms forward selection, but with $n$ increasing, forward selection starts to outperform Lasso model on RMSE and variance. And overall, Lasso tends to perform better when there are more strong signals. If there are more strong predictors, the variance is also larger.

## Effect of Missing Weak Predictors

Here we define missing weak predictors as the true weak predictors but we estimated them as null predictors, and we used RMSE to evaluate. In each scenario, we picked up three kinds of missing situations: most missing, least missing, and middle missing. As the name suggests, the most missing are the ones that have the least non-null estimations. Foe each kind of situation, we picked 10% to draw the plot. For example, we fixed 100 parameters and 100 simulation times. In each simulation, we can get the number of non-null parameters. After arranging them, we can pick the top 10 simulation times that have the most non-null parameters. In the same way, we can pick the last 10 and the middle 10 which ranks 46 to 55. It is worth mentioning that, Lasso only picks which parameters to use and the coefficients of Lasso can not be used directly. In order to compare RMSE, we need to refit the linear regression model using the parameters that Lasso picks.

When in high dimensional scenario ($n=100$), Lasso has a very small RMSE however forward selection’s RMSE is big. But when the missing amount increases, the RMSE of forward selection drops dramatically. When the number of missing parameters increases, the RMSEs of both methods decrease. 

When under normal scenario ($n = 500$ or 2000), the RMSEs of both methods are small. It's hard to tell which method is better since their differences are small. There seems to be no apparent patterns between different ratios as well.

Since we care more about high dimensional scenario, the conclusion should be Lasso performs better than forward selection when in high dimensional scenario according to RMSE, and the more missing parameters, the better the RMSE.

# Discussions

## Limitations

There is much freedom when designing the simulations. In our algorithm, we have 5 parameters: number of observations, number of parameters, the ratio of strong and weak signals, the definition of strong and weak signals and the correlation between strong and weak signals. However, even more parameters can be adjusted such as the correlation within WAI signals, or between null and strong signals, etc. We generated many versions of data and found that many things can affect the result. Here we only fixed $p$ and $c$ and the results and conclusions may not be comprehensive.

## Future Work

For the future work, we could adjust other parameters to investigate this problem further. What's more, we reproduced the high-dimensional scenario and faced the struggle of choosing covariates. We still could not have a clear solution to deal with this difficulty. It would be hard to tackle the problem, but it can be a direction of effort.

# Contributions  {-}

PLEASE INSERT CONTRIBUTIONS HERE.


Shaohan Chen mainly focused himself on task 1. He conducted model evaluation based on the simulation code and simulated data generated by Hongjie Liu, and gave visualization analysis on the signals identification performance as well as the parameter estimation performance of both models. He was also responsible for giving presentation of task 1 and writing the corresponding parts of the slides and report.

# Reference  {-}

1. Li Y, Hong HG, Ahmed SE, Li Y. Weak signals in high‐dimensional regression: Detection, estimation and prediction. Appl Stochastic Models Bus Ind. 2018;1–16. https://doi.org/10.1002/asmb.2340

\pagebreak

# Appendix {-}

```{r, include=FALSE, warning = FALSE}
# Load parameter data
param_1 = read_csv("parameter_estimate.csv")
param_2 = read_csv("parameter_estimate2.csv")

param_1 =
  param_1 %>%
  filter(method %in% c("forward", "lasso_lmfit")) %>%
  mutate(method = 
           case_when(
             method == "forward" ~ "forward",
             method == "lasso_lmfit" ~ "lasso"
           ))
param_2 =
  param_2 %>%
  filter(method %in% c("forward", "lasso_lmfit")) %>%
  mutate(method = 
           case_when(
             method == "forward" ~ "forward",
             method == "lasso_lmfit" ~ "lasso"
           ))

param_estimate = rbind(param_1, param_2)


# Output identification table
fit_identify_result = 
  param_estimate %>% 
    mutate(
      n = factor(n),
      ratio = ifelse(ratio == 145, "1:4:5", "3:3:4"),
      notnull = ifelse(type == "null", 0, 1),
      notnull_est = ifelse(abs(estimate) > 0, 1, 0),
      identify =
        case_when(
          notnull == 1 & notnull_est == 1 ~ "TP",
          notnull == 1 & notnull_est == 0 ~ "FN",
          notnull == 0 & notnull_est == 1 ~ "FP",
          notnull == 0 & notnull_est == 0 ~ "TN"
        )
    ) %>% 
    group_by(sim_time, n, p, ratio, method) %>% 
    summarize(
      number_of_parameters = sum(identify == "TP") + sum(identify == "FP"),
      sensitivity = sum(identify == "TP")/(sum(identify == "TP") + sum(identify == "FN")),
      specificity = sum(identify == "TN")/(sum(identify == "TN") + sum(identify == "FP")),
      precision = sum(identify == "TP")/(sum(identify == "TP") + sum(identify == "FP")),
      accuracy = (sum(identify == "TP") + sum(identify == "TN")) /
        (sum(identify == "TP") + sum(identify == "FP") + sum(identify == "TN") + sum(identify == "FN")),
      F1_score = 2 * precision * sensitivity / (precision + sensitivity)
    )


# Output identification table by signal
fit_identify_result_type = 
  param_estimate %>% 
    mutate(
      n = factor(n),
      ratio = ifelse(ratio == 145, "1:4:5", "3:3:4"),
      notnull = ifelse(type == "null", 0, 1),
      notnull_est = ifelse(abs(estimate) > 0, 1, 0),
      identify =
        case_when(
          notnull == 1 & notnull_est == 1 ~ "TP",
          notnull == 1 & notnull_est == 0 ~ "FN",
          notnull == 0 & notnull_est == 1 ~ "FP",
          notnull == 0 & notnull_est == 0 ~ "TN"
        )
    ) %>% 
    group_by(sim_time, n, p, ratio, method, type) %>% 
    summarize(
      number_of_parameters = sum(identify == "TP") + sum(identify == "FP"),
      sensitivity = sum(identify == "TP")/(sum(identify == "TP") + sum(identify == "FN")),
      specificity = sum(identify == "TN")/(sum(identify == "TN") + sum(identify == "FP")),
      precision = sum(identify == "TP")/(sum(identify == "TP") + sum(identify == "FP")),
      accuracy = (sum(identify == "TP") + sum(identify == "TN")) /
        (sum(identify == "TP") + sum(identify == "FP") + sum(identify == "TN") + sum(identify == "FN")),
      F1_score = 2 * precision * sensitivity / (precision + sensitivity)
    )


p = 100
ratio = c(1, 4, 5)
ps = floor(0.4*p*ratio[1]/sum(ratio))
pwbc = floor(0.4*p*ratio[2]/sum(ratio))
pwai = floor(0.4*p*ratio[3]/sum(ratio))
beta = c(rep(20, ps), rep(0.5, pwbc + pwai), rep(0, p - ps - pwbc - pwai))
# output the beta estimation table of ratio 1
fit_beta_result_ratio1 = 
  param_1 %>% 
    mutate(
      n = factor(n),
      ratio = ifelse(ratio == 145, "1:4:5", "3:3:4"),
      num = case_when(
  type == "strong" ~ num,
  type == "wbc" ~ num + ps,
  type == "wai" ~ num + ps + pwbc,
  type == "null" ~ num + ps + pwbc + pwai),
      se = (estimate - beta[num])^2,
      var = (estimate - mean(estimate))^2
    ) %>% 
    group_by(sim_time, n, p, ratio, method) %>% 
    summarize(
      RMSE = sqrt(mean(se)),
      Variance = mean(var)
    )

# output the beta estimation table of ratio2
p = 100
ratio = c(3, 3, 4)
ps = floor(0.4*p*ratio[1]/sum(ratio))
pwbc = floor(0.4*p*ratio[2]/sum(ratio))
pwai = floor(0.4*p*ratio[3]/sum(ratio))
beta = c(rep(20, ps), rep(0.5, pwbc + pwai), rep(0, p - ps - pwbc - pwai))

fit_beta_result_ratio2 = 
  param_2 %>% 
    mutate(
      n = factor(n),
      ratio = ifelse(ratio == 145, "1:4:5", "3:3:4"),
      num = case_when(
  type == "strong" ~ num,
  type == "wbc" ~ num + ps,
  type == "wai" ~ num + ps + pwbc,
  type == "null" ~ num + ps + pwbc + pwai),
      se = (estimate - beta[num])^2,
      var = (estimate - mean(estimate))^2
    ) %>% 
    group_by(sim_time, n, p, ratio, method) %>% 
    summarize(
      RMSE = sqrt(mean(se)),
      Variance = mean(var)
    )

fit_beta_result_ratio = 
  rbind(fit_beta_result_ratio1, fit_beta_result_ratio2)
```

```{r, include  = FALSE}
# compare model complexity

# compare sensitivity
g_1 = ggplot(fit_identify_result, aes(x = method, y = number_of_parameters, color = n)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Model Complexity",x = "method", y = "Number of Selected Parameters") +
  facet_grid(. ~ ratio)
```


```{r, include  = FALSE}
# compare all signals on four metrics

# compare sensitivity
g1 = ggplot(fit_identify_result, aes(x = method, y = sensitivity, color = n)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Sensitivity",x = "method", y = "sensitivity") +
  facet_grid(. ~ ratio)

# compare specificity
g2 = ggplot(fit_identify_result, aes(x = method, y = specificity, color = n)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Specificity",x = "method", y = "specificity") +
  facet_grid(. ~ ratio)

# compare F1-score
g3 = ggplot(fit_identify_result, aes(x = method, y = F1_score, color = n)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "F1_score",x = "method", y = "F1_score") +
  facet_grid(. ~ ratio)

# compare accuracy
g4 = ggplot(fit_identify_result, aes(x = method, y = accuracy, color = n)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Accuracy",x = "method", y = "accuracy") +
  facet_grid(. ~ ratio)


# compare different signals

# First ratio
g5 = ggplot(fit_identify_result_type, aes(x = method, y = sensitivity, color = type)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Sensitivity",x = "method", y = "sensitivity") +
  facet_grid(ratio ~ n)

g6 = ggplot(fit_identify_result_type, aes(x = method, y = specificity, color = type)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Specificity",x = "method", y = "specificity") +
  facet_grid(ratio ~ n)

g7 = ggplot(fit_identify_result_type, aes(x = method, y = F1_score, color = type)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "F1_score",x = "method", y = "F1_score") +
  facet_grid(ratio ~ n)

g8 = ggplot(fit_identify_result_type, aes(x = method, y = accuracy, color = type)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Accuracy",x = "method", y = "accuracy") +
  facet_grid(ratio ~ n)


# Beta estimation of RMSE

# RMSE of beta for n = 100
g9 = ggplot(fit_beta_result_ratio %>% filter(n == 100), aes(x = method, y = RMSE)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta RMSE when n = 100",x = "method", y = "RMSE") +
  coord_cartesian(ylim = c(0, 15)) +
  facet_grid(~ ratio)

# RMSE of beta for n = 500
g10 = ggplot(fit_beta_result_ratio %>% filter(n == 500), aes(x = method, y = RMSE)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta RMSE when n = 500",x = "method", y = "RMSE") +
  coord_cartesian(ylim = c(0, 0.6)) +
  facet_grid(~ ratio)

# RMSE of beta for n = 2000
g11 = ggplot(fit_beta_result_ratio %>% filter(n == 2000), aes(x = method, y = RMSE)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta RMSE when n = 2000",x = "method", y = "RMSE") +
  coord_cartesian(ylim = c(0, 0.4)) +
  facet_grid(~ ratio)


# Beta estimation of Variance

# Variance of beta for n = 100
g12 = ggplot(fit_beta_result_ratio %>% filter(n == 100), aes(x = method, y = Variance)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta Variance when n = 100",x = "method", y = "Variance") +
  coord_cartesian(ylim = c(0, 150)) +
  facet_grid(~ ratio)

# Variance of beta for n = 500
g13 = ggplot(fit_beta_result_ratio %>% filter(n == 500), aes(x = method, y = Variance)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta Variance when n = 500",x = "method", y = "Variance") +
  coord_cartesian(ylim = c(0, 50)) +
  facet_grid(~ ratio)

# Variance of beta for n = 2000
g14 = ggplot(fit_beta_result_ratio %>% filter(n == 2000), aes(x = method, y = Variance)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta Variance when n = 2000",x = "method", y = "Variance") +
  coord_cartesian(ylim = c(0, 150)) +
  facet_grid(~ ratio)
```

```{r, include  = FALSE}
# generate RMSE for ratio 145
beta = c(rep(20, 4), rep(0.5, 36), rep(0, 60))

get_MSE <- function(sim_number,what_number,what_method){
  estimate = 
    read_csv("parameter_estimate.csv",show_col_types = FALSE) %>%
    filter(n == what_number & method == what_method &sim_time == sim_number) %>% 
    mutate(type = factor(type, levels = c('strong', 'wbc', 'wai', 'null'))) %>% 
    arrange(type) %>% 
    pull(estimate)
  
  RMSE = sqrt(sum((estimate - beta) ^2) / 100)
  RMSE
}

getresulttable_145 <- function(what_number,what_method){
  param_estimate = 
    read_csv("parameter_estimate.csv",show_col_types = FALSE) %>%
    filter(n == what_number & method == what_method)

non_null = 
  param_estimate %>% 
  filter(estimate != 0)

non_null %>% 
  group_by(sim_time) %>% 
  summarise(
  n_param = n()
) %>% 
  arrange(-n_param) %>%
  pull(n_param)

sum = 
  non_null %>%
  group_by(sim_time) %>%
  summarise(n_param = n()) %>% 
  arrange(-n_param)

max_sims = head(sum, 10) %>% pull(sim_time)
min_sims = tail(sum, 10) %>% pull(sim_time)
middle_sims = head(sum, 55) %>% tail(10) %>% pull(sim_time)

most = pmap(list(min_sims,what_number,what_method), get_MSE) %>% unlist()
least = pmap(list(max_sims,what_number,what_method), get_MSE) %>% unlist()
middle = pmap(list(middle_sims,what_number,what_method), get_MSE) %>% unlist()

result = 
  tibble(most, middle,least) %>% 
  pivot_longer(
    most:least,
    names_to = "missing",
    values_to = "RMSE"
    ) 

result = result %>% 
  add_column(n = c(rep(what_number, 30)), method = c(rep(what_method, 30)), ratio = c(rep("1:4:5", 30)))
result
}

forward_100_145 = getresulttable_145(what_number = 100, what_method = "forward")
forward_500_145 = getresulttable_145(what_number = 500, what_method = "forward")
forward_2000_145 = getresulttable_145(what_number = 2000, what_method = "forward")
lasso_100_145 = getresulttable_145(what_number = 100, what_method = "lasso_lmfit")
lasso_500_145 = getresulttable_145(what_number = 500, what_method = "lasso_lmfit")
lasso_2000_145 = getresulttable_145(what_number = 2000, what_method = "lasso_lmfit")

# generate RMSE for ratio 334
beta = c(rep(20, 12), rep(0.5, 28), rep(0, 60))

get_MSE <- function(sim_number,what_number,what_method){
  estimate = 
    read_csv("parameter_estimate2.csv",show_col_types = FALSE) %>%
    filter(n == what_number & method == what_method &sim_time == sim_number) %>% 
    mutate(type = factor(type, levels = c('strong', 'wbc', 'wai', 'null'))) %>% 
    arrange(type) %>% 
    pull(estimate)
  
  RMSE = sqrt(sum((estimate - beta) ^2) / 100)
  RMSE
}

getresulttable_334 <- function(what_number,what_method){
  param_estimate = 
    read_csv("parameter_estimate2.csv",show_col_types = FALSE) %>%
    filter(n == what_number & method == what_method)

non_null = 
  param_estimate %>% 
  filter(estimate != 0)

non_null %>% 
  group_by(sim_time) %>% 
  summarise(
  n_param = n()
) %>% 
  arrange(-n_param) %>%
  pull(n_param)

sum = 
  non_null %>%
  group_by(sim_time) %>%
  summarise(n_param = n()) %>% 
  arrange(-n_param)

max_sims = head(sum, 10) %>% pull(sim_time)
min_sims = tail(sum, 10) %>% pull(sim_time)
middle_sims = head(sum, 55) %>% tail(10) %>% pull(sim_time)

most = pmap(list(min_sims,what_number,what_method), get_MSE) %>% unlist()
least = pmap(list(max_sims,what_number,what_method), get_MSE) %>% unlist()
middle = pmap(list(middle_sims,what_number,what_method), get_MSE) %>% unlist()

result = 
  tibble(most, middle,least) %>% 
  pivot_longer(
    most:least,
    names_to = "missing",
    values_to = "RMSE"
    ) 

result = result %>% 
  add_column(n = c(rep(what_number, 30)), method = c(rep(what_method, 30)), ratio = c(rep("3:3:4", 30)))
result
}

forward_100_334 = getresulttable_334(what_number = 100, what_method = "forward")
forward_500_334 = getresulttable_334(what_number = 500, what_method = "forward")
forward_2000_334 = getresulttable_334(what_number = 2000, what_method = "forward")
lasso_100_334 = getresulttable_334(what_number = 100, what_method = "lasso_lmfit")
lasso_500_334 = getresulttable_334(what_number = 500, what_method = "lasso_lmfit")
lasso_2000_334 = getresulttable_334(what_number = 2000, what_method = "lasso_lmfit")

result = 
  forward_100_145 %>% 
  add_row(forward_500_145) %>% 
  add_row(forward_2000_145) %>% 
  add_row(lasso_100_145) %>% 
  add_row(lasso_500_145) %>% 
  add_row(lasso_2000_145) %>% 
  add_row(forward_100_334) %>% 
  add_row(forward_500_334) %>% 
  add_row(forward_2000_334) %>% 
  add_row(lasso_100_334) %>% 
  add_row(lasso_500_334) %>% 
  add_row(lasso_2000_334)

m1 = ggplot(filter(result, n == 100), aes(x = missing, y = RMSE, color = method)) + 
  geom_boxplot() +
  ylim(0,5) +
  theme(legend.position = "right") +
  labs(title = "RMSE",x = "missing", y = "RMSE") +
  facet_grid(. ~ ratio)

m2 = ggplot(filter(result, n == 500), aes(x = missing, y = RMSE, color = method)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "RMSE",x = "missing", y = "RMSE") +
  facet_grid(. ~ ratio)
  
m3 = ggplot(filter(result, n == 2000), aes(x = missing, y = RMSE, color = method)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "RMSE",x = "missing", y = "RMSE") +
  facet_grid(. ~ ratio)

```

```{r, echo = FALSE, fig.cap = "Model Complexity", fig.height = 7,fig.width = 7,warning=FALSE}
g_1
```

```{r, echo = FALSE, fig.cap = "Overall Classification Performance", fig.height = 8,fig.width = 9,warning=FALSE}
(g1 + g2) / (g3 + g4)
```

```{r, echo = FALSE, fig.cap = "Classification Performance by Signals", fig.height = 8,fig.width = 9,warning=FALSE}
(g5 + g6) / (g7 + g8)
```

```{r, echo = FALSE, fig.cap = "Parameter Estimation Performance", fig.height = 8,fig.width = 9,warning=FALSE}
(g9 + g12) / (g10 + g13) / (g11 + g14)
```

```{r, echo = FALSE, fig.cap = "RMSE comparison when n = 100", fig.height = 8,fig.width = 9, warning=FALSE}
m1
```

```{r, echo = FALSE, fig.cap = "RMSE comparison when n = 500", fig.height = 8,fig.width = 9}
m2
```

```{r, echo = FALSE, fig.cap = "RMSE comparison when n = 2000", fig.height = 8,fig.width = 9}
m3
```