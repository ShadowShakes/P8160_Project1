---
title: "Variable Selection Methods Comparison"
author: "Hongjie Liu, Jiajun Tao, Shaohan Chen"
header-includes:
   - \usepackage{graphicx}
   - \usepackage{float}
   - \usepackage{subfigure}
output:
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gridExtra)
library(tidyverse)
library(patchwork)
library(dplyr)
```


# Background and Objectives

## Background

In linear models, when facing high-dimensional data, variable selection method is a common practice to achieve a balance between model fitness and complexity. However, in modern high-dimensional data applications, traditional variable selection methods often struggle with the presence of "weak" predictors, i.e., predictors with small but non-zero coefficients.

## Objectives

This project aims to compare two popular variable selection methods, the step-wise forward method using the Akaike information criterion (AIC) and the automated LASSO regression. We conducted simulations under scenarios with high-dimensional data and larger numbers of observations to investigate how well each method performs in identifying weak and strong predictors and how missing weak predictors affects parameter estimations.

# Statistical Methods to be Studied

## Step-wise forward method

The step-wise forward method is an iterative process that starts with an empty model and sequentially adds variables that best improve the model fit, usually by adding predictors with the largest reduction in AIC. For linear models,
$$AIC = n\log\left(\sum_{i=1}^n (y_i - \widehat{y}_i)^2/n\right) + 2p,$$
where $\widehat{y}_i$ is the fitted values from a model, and $p$ is the dimension of the model.

## Automated LASSO regression
  
Automated LASSO regression estimates the model parameters by optimizing a penalized loss function:
$$\min_{\boldsymbol{\beta}} \frac{1}{2n} \sum_{i=1}^n \left(y_i - \beta_0 - \sum_{j=1}^p \beta_i x_{ij} \right)^2 + \lambda \sum_{k=1}^p|\beta_k|,$$
where $\lambda\geq0$ is a tuning parameter chosen by cross validations.

# Scenarios to be Investigated

## Types of Signals

Strong signals
$$S_{strong}=\{j:|\beta_j|>c\sqrt{\log (p) / n}\ \mbox{for some } c>0,\  1\le j \le p\}$$
Weak-but-correlated (WBC) signals
$$S_{WBC}=\ \{j: 0<|\beta_j|\le c\sqrt{\log (p) / n} \text{ and } \mbox{corr}(X_j, X_j')\ne 0\mbox{ for some } j'\in S_1,\  1\le j \le p\}$$
Weak-and-independent (WAI) signals
$$S_{WAI}=\ \{j: 0<|\beta_j|\le c\sqrt{\log (p) / n} \text{ and } \mbox{corr}(X_j, X_j')= 0\mbox{ for all } j'\in S_1, \ 1\le j \le p\}$$
Null signals
$$S_{null}=\{j: \beta_j=0,\  1\le j \le p\}$$

Thus, $p$ predictors can be partitioned as $$\{1,\cdots,p\}=S_{strong}\cup S_{WBC}\cup S_{WAI}\cup S_{null}.$$ We assume that $|S_{strong}|=p_{S}$, $|S_{WBC}|=p_{WBC}$, $|S_{WAI}|=p_{WAI}$. The number of true predictors $p_{S}+p_{WBC}+p_{WAI}$ should be less than $n$.

## PLSASE INSERT SENARIOS HERE


# Methods for Data Generation

## Response Vector

Normality assumption: $$\mathbf{y}\sim N(\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})$$

For $j\in S_{strong}$, $\beta_j=20$; For $j\in S_{WBC}\cup S_{WAI}$, $\beta_j=0.5$. We choose $c=20$ so that $0.5\leq c\sqrt{\log p/n} < 20$ for all scenarios to be investigated. For the error term, we set $\sigma = 8.$


## Design Matrix

We assume $$\mathbf{x}_i\sim N(\boldsymbol{\mu},\boldsymbol{\Sigma}),\ i=1,\cdots,n$$

All predictors are standardized. Then we have $\boldsymbol{\mu}=\mathbf{0}$ and $\Sigma_{i,i}=1$ for all $i$. We set $p_{WBC}\geq p_{strong}$. For each strong predictor (except one of them), we set $[p_{WBC}/p_{strong}]$ WBC predictors to be correlated with it. Each WBC predictor is set to be correlated with one and only one strong predictor. All other elements of $\boldsymbol{\Sigma}$ are 0. We use the `MASS::mvrnorm` function to generate data following a multivariate normal distribution.


# Model Evaluation

## Evaluation Metrics
In this project, we define the true predictors as positive and null predictors as negative.\par
For signal identification, we use the following five metrics to compare the two models:\par
- Complexity: The number of selected predictors in the model
- Sensitivity: $\frac{TP}{TP+FN}$
- Specificity: $\frac{TN}{TN+FP}$
- F1-score: $\frac{2\cdot sensitivity\cdot precision}{sensitivity + precision}$
- Accuracy: $\frac{TP+TN}{TP+TN+FP+FN}$\par

For parameter estimation, we use the following to metrics to compare the two models:\par
- RMSE: $\sqrt{\frac{1}{p}\sum_{i=1}^p (\hat{\beta_i}-\beta_i)^2}$
- Variance: $\frac{1}{p}\sum_{i=1}^p (\hat{\beta_i}-\bar{\beta_i})^2$

## Signal Indentification Performance



Complexity of the models is indicated by the number of selected predictors. We can see that in high dimensional scenario(when n=100), Forward selection model tends to select lots of predictors and Lasso tends to select very few. One interesting thing is that, if we increase the ratio of strong predictors(i.e. more strong predictors), Lasso also tends to select more predictors too. When it comes to normal scenario, Forward selection still tends to select more predictors than Lasso, but the discrepancy is smaller than high dimensional case, and will be further narrowed down with n or the ratio of strong predictors increasing. And as n increases, the number of selected parameters of both models are closer to the true number 40.

As for overall classification performance, if in high dimensional scenario, Forward selection tends to be very assertive and much better at identifying weak signals, leading to an extremely high sensitivity but low specificity. Lasso in turn tends to be very conservative and much better at identifying null signals, leading to an extremely high specificity but low sensitivity. Like high dimensional case, Lasso becomes more sensitive and not that assertive when ratio of strong predictors increases. Based on the above plot, we can conclude that both models does not perform too well based on F1-score and accuracy, because they are very radical and tend to identify most of the predictors either as positive or negative, but far away from the truth.\par
For normal scenario, both models become less radical under normal scenarios, but Forward selection is still more sensitive and assertive than Lasso, while Lasso has higher specificity and more conservative. Both models perform better on those metrics with n increasing. Overall, Lasso and Forward selection has similar F1-score and accuracy performance. But when there are more strong predictors, Lasso performs obviously better than Forward selection.

About the classification performance of different signals. Under all n values, both models perfectly identify the strong signals. In high dimensional scenario, Forward selection performs much better on identifying weak predictors while Lasso performs much better on identifying null predictors, that’ s why we see the high sensitivity of forward selection and high specificity of Lasso in previous section. When there are more strong predictors, Lasso also performs better on selecting weak predictors. When it comes to normal scenario, Forward selection is still better at selecting null and Lasso is better at selecting weak predictors. But the discrepancy is smaller compared with high-dimensional data, and will continue be smaller as n increases. When there are more strong predictors, Lasso performs much better at selecting weak-but-correlated signals.

## Parameter Estimatio Performance

In high dimensional scenario, Lasso performs much better than forward selection, with obvious much lower and centered RMSE and also lower variance. When it comes to normal scenario. Though when n=500, Lasso outperforms forward selection, but with n increasing, forward selection starts to outperform Lasso model on RMSE and variance. And overall, Lasso tends to perform better when there are more strong signals. If there are more strong predictors, the variance is also larger.


## Effect of Missing Weak Predictors

Here we define missing weak predictors as the true weak predictors but we estimated them as null predictors, and we used RMSE to evaluate. In each scenario, we picked up three kinds of missing situations i.e. most missing, least missing, and middle missing. As the name suggests, the most missing are the ones that have the least non-null estimations. Foe each kind of situation, we picked 10% to draw the plot. For example, we fixed 100 parameters and 100 simulation times. In each simulation, we can get the number of non-null parameters. After arranging them, we can pick the top 10 simulation times that have the most non-null parameters. In the same way, we can pick the last 10 and the middle 10 which ranks 45 to 55. It is worth mentioning that, Lasso only picks which parameters to use and the coefficients of Lasso can not be used directly. In order to compare RMSE, we need to refit the linear regression model using the parameters that Lasso picks.

When in high dimensional scenario(when n=100), Lasso has a very small RMSE however forward selection’s RMSE is big. But when the missing amount increases, the RMSE of forward selection drops dramatically. When the number of missing parameters increases, the RMSEs of both methods decrease. 

When under normal scenario(n = 500 or 2000), the RMSEs of both methods are small. It's hard to tell which method is better since their differences are small. There seems to be no apparent patterns between different ratios as well.

Since we care more about high dimensional scenario, the conclusion should be Lasso performs better than forward selection when in high dimensional scenario according to RMSE, and the more missing parameters, the better the RMSE.

# Discussions

## Limitation

There is much freedom when designing the simulations. In our algorithm, we have 5 parameters, number of observations, number of parameters, the ratio of strong and weak signals, the definition of strong and weak signals and the correlation between strong and weak signals. However, even more parameters can be adjusted such as the correlation between WBC and WAI, or between null and strong, etc. We generated many versions of data and found that many things can affect the result. Here we only fixed p and c and the results and conclusions may not be comprehensive.

## Future Work

For the future work, we could adjust other parameters to investigate this problem further. What's more, we reproduced the high dimensional scenario and faced the stuggle of choosing covariates. We still could not have a clear solution to deal with this difficulty. It would be hard to tackle the problem, but it can be a direction of effort.

# Contributions  {-}

PLEASE INSERT CONTRIBUTIONS HERE.

# Reference  {-}

1. Li Y, Hong HG, Ahmed SE, Li Y. Weak signals in high‐dimensional regression: Detection, estimation and prediction. Appl Stochastic Models Bus Ind. 2018;1–16. https://doi.org/10.1002/asmb.2340

\pagebreak

# Appendix {-}

```{r, include=FALSE, warning = FALSE}
# Load parameter data
param_1 = read_csv("parameter_estimate.csv")
param_2 = read_csv("parameter_estimate2.csv")

param_1 =
  param_1 %>%
  filter(method %in% c("forward", "lasso_lmfit")) %>%
  mutate(method = 
           case_when(
             method == "forward" ~ "forward",
             method == "lasso_lmfit" ~ "lasso"
           ))
param_2 =
  param_2 %>%
  filter(method %in% c("forward", "lasso_lmfit")) %>%
  mutate(method = 
           case_when(
             method == "forward" ~ "forward",
             method == "lasso_lmfit" ~ "lasso"
           ))

param_estimate = rbind(param_1, param_2)


# Output identification table
fit_identify_result = 
  param_estimate %>% 
    mutate(
      n = factor(n),
      ratio = ifelse(ratio == 145, "1:4:5", "3:3:4"),
      notnull = ifelse(type == "null", 0, 1),
      notnull_est = ifelse(abs(estimate) > 0, 1, 0),
      identify =
        case_when(
          notnull == 1 & notnull_est == 1 ~ "TP",
          notnull == 1 & notnull_est == 0 ~ "FN",
          notnull == 0 & notnull_est == 1 ~ "FP",
          notnull == 0 & notnull_est == 0 ~ "TN"
        )
    ) %>% 
    group_by(sim_time, n, p, ratio, method) %>% 
    summarize(
      number_of_parameters = sum(identify == "TP") + sum(identify == "FP"),
      sensitivity = sum(identify == "TP")/(sum(identify == "TP") + sum(identify == "FN")),
      specificity = sum(identify == "TN")/(sum(identify == "TN") + sum(identify == "FP")),
      precision = sum(identify == "TP")/(sum(identify == "TP") + sum(identify == "FP")),
      accuracy = (sum(identify == "TP") + sum(identify == "TN")) /
        (sum(identify == "TP") + sum(identify == "FP") + sum(identify == "TN") + sum(identify == "FN")),
      F1_score = 2 * precision * sensitivity / (precision + sensitivity)
    )


# Output identification table by signal
fit_identify_result_type = 
  param_estimate %>% 
    mutate(
      n = factor(n),
      ratio = ifelse(ratio == 145, "1:4:5", "3:3:4"),
      notnull = ifelse(type == "null", 0, 1),
      notnull_est = ifelse(abs(estimate) > 0, 1, 0),
      identify =
        case_when(
          notnull == 1 & notnull_est == 1 ~ "TP",
          notnull == 1 & notnull_est == 0 ~ "FN",
          notnull == 0 & notnull_est == 1 ~ "FP",
          notnull == 0 & notnull_est == 0 ~ "TN"
        )
    ) %>% 
    group_by(sim_time, n, p, ratio, method, type) %>% 
    summarize(
      number_of_parameters = sum(identify == "TP") + sum(identify == "FP"),
      sensitivity = sum(identify == "TP")/(sum(identify == "TP") + sum(identify == "FN")),
      specificity = sum(identify == "TN")/(sum(identify == "TN") + sum(identify == "FP")),
      precision = sum(identify == "TP")/(sum(identify == "TP") + sum(identify == "FP")),
      accuracy = (sum(identify == "TP") + sum(identify == "TN")) /
        (sum(identify == "TP") + sum(identify == "FP") + sum(identify == "TN") + sum(identify == "FN")),
      F1_score = 2 * precision * sensitivity / (precision + sensitivity)
    )


p = 100
ratio = c(1, 4, 5)
ps = floor(0.4*p*ratio[1]/sum(ratio))
pwbc = floor(0.4*p*ratio[2]/sum(ratio))
pwai = floor(0.4*p*ratio[3]/sum(ratio))
beta = c(rep(20, ps), rep(0.5, pwbc + pwai), rep(0, p - ps - pwbc - pwai))
# output the beta estimation table of ratio 1
fit_beta_result_ratio1 = 
  param_1 %>% 
    mutate(
      n = factor(n),
      ratio = ifelse(ratio == 145, "1:4:5", "3:3:4"),
      num = case_when(
  type == "strong" ~ num,
  type == "wbc" ~ num + ps,
  type == "wai" ~ num + ps + pwbc,
  type == "null" ~ num + ps + pwbc + pwai),
      se = (estimate - beta[num])^2,
      var = (estimate - mean(estimate))^2
    ) %>% 
    group_by(sim_time, n, p, ratio, method) %>% 
    summarize(
      RMSE = sqrt(mean(se)),
      Variance = mean(var)
    )

# output the beta estimation table of ratio2
p = 100
ratio = c(3, 3, 4)
ps = floor(0.4*p*ratio[1]/sum(ratio))
pwbc = floor(0.4*p*ratio[2]/sum(ratio))
pwai = floor(0.4*p*ratio[3]/sum(ratio))
beta = c(rep(20, ps), rep(0.5, pwbc + pwai), rep(0, p - ps - pwbc - pwai))

fit_beta_result_ratio2 = 
  param_2 %>% 
    mutate(
      n = factor(n),
      ratio = ifelse(ratio == 145, "1:4:5", "3:3:4"),
      num = case_when(
  type == "strong" ~ num,
  type == "wbc" ~ num + ps,
  type == "wai" ~ num + ps + pwbc,
  type == "null" ~ num + ps + pwbc + pwai),
      se = (estimate - beta[num])^2,
      var = (estimate - mean(estimate))^2
    ) %>% 
    group_by(sim_time, n, p, ratio, method) %>% 
    summarize(
      RMSE = sqrt(mean(se)),
      Variance = mean(var)
    )

fit_beta_result_ratio = 
  rbind(fit_beta_result_ratio1, fit_beta_result_ratio2)
```

```{r, include  = FALSE}
# compare model complexity

# compare sensitivity
g_1 = ggplot(fit_identify_result, aes(x = method, y = number_of_parameters, color = n)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Model Complexity",x = "method", y = "Number of Selected Parameters") +
  facet_grid(. ~ ratio)
```


```{r, include  = FALSE}
# compare all signals on four metrics

# compare sensitivity
g1 = ggplot(fit_identify_result, aes(x = method, y = sensitivity, color = n)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Sensitivity",x = "method", y = "sensitivity") +
  facet_grid(. ~ ratio)

# compare specificity
g2 = ggplot(fit_identify_result, aes(x = method, y = specificity, color = n)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Specificity",x = "method", y = "specificity") +
  facet_grid(. ~ ratio)

# compare F1-score
g3 = ggplot(fit_identify_result, aes(x = method, y = F1_score, color = n)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "F1_score",x = "method", y = "F1_score") +
  facet_grid(. ~ ratio)

# compare accuracy
g4 = ggplot(fit_identify_result, aes(x = method, y = accuracy, color = n)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Accuracy",x = "method", y = "accuracy") +
  facet_grid(. ~ ratio)


# compare different signals

# First ratio
g5 = ggplot(fit_identify_result_type, aes(x = method, y = sensitivity, color = type)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Sensitivity",x = "method", y = "sensitivity") +
  facet_grid(ratio ~ n)

g6 = ggplot(fit_identify_result_type, aes(x = method, y = specificity, color = type)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Specificity",x = "method", y = "specificity") +
  facet_grid(ratio ~ n)

g7 = ggplot(fit_identify_result_type, aes(x = method, y = F1_score, color = type)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "F1_score",x = "method", y = "F1_score") +
  facet_grid(ratio ~ n)

g8 = ggplot(fit_identify_result_type, aes(x = method, y = accuracy, color = type)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "Accuracy",x = "method", y = "accuracy") +
  facet_grid(ratio ~ n)


# Beta estimation of RMSE

# RMSE of beta for n = 100
g9 = ggplot(fit_beta_result_ratio %>% filter(n == 100), aes(x = method, y = RMSE)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta RMSE when n = 100",x = "method", y = "RMSE") +
  coord_cartesian(ylim = c(0, 15)) +
  facet_grid(~ ratio)

# RMSE of beta for n = 500
g10 = ggplot(fit_beta_result_ratio %>% filter(n == 500), aes(x = method, y = RMSE)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta RMSE when n = 500",x = "method", y = "RMSE") +
  coord_cartesian(ylim = c(0, 0.6)) +
  facet_grid(~ ratio)

# RMSE of beta for n = 2000
g11 = ggplot(fit_beta_result_ratio %>% filter(n == 2000), aes(x = method, y = RMSE)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta RMSE when n = 2000",x = "method", y = "RMSE") +
  coord_cartesian(ylim = c(0, 0.4)) +
  facet_grid(~ ratio)


# Beta estimation of Variance

# Variance of beta for n = 100
g12 = ggplot(fit_beta_result_ratio %>% filter(n == 100), aes(x = method, y = Variance)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta Variance when n = 100",x = "method", y = "Variance") +
  coord_cartesian(ylim = c(0, 150)) +
  facet_grid(~ ratio)

# Variance of beta for n = 500
g13 = ggplot(fit_beta_result_ratio %>% filter(n == 500), aes(x = method, y = Variance)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta Variance when n = 500",x = "method", y = "Variance") +
  coord_cartesian(ylim = c(0, 50)) +
  facet_grid(~ ratio)

# Variance of beta for n = 2000
g14 = ggplot(fit_beta_result_ratio %>% filter(n == 2000), aes(x = method, y = Variance)) + 
  geom_boxplot(color = "red", fill = "orange", alpha = 0.5) +
  theme(legend.position = "right") +
  labs(title = "beta Variance when n = 2000",x = "method", y = "Variance") +
  coord_cartesian(ylim = c(0, 150)) +
  facet_grid(~ ratio)
```

```{r, include  = FALSE}
# generate RMSE for ratio 145
beta = c(rep(20, 4), rep(0.5, 36), rep(0, 60))

get_MSE <- function(sim_number,what_number,what_method){
  estimate = 
    read_csv("parameter_estimate.csv",show_col_types = FALSE) %>%
    filter(n == what_number & method == what_method &sim_time == sim_number) %>% 
    mutate(type = factor(type, levels = c('strong', 'wbc', 'wai', 'null'))) %>% 
    arrange(type) %>% 
    pull(estimate)
  
  RMSE = sqrt(sum((estimate - beta) ^2) / 100)
  RMSE
}

getresulttable_145 <- function(what_number,what_method){
  param_estimate = 
    read_csv("parameter_estimate.csv",show_col_types = FALSE) %>%
    filter(n == what_number & method == what_method)

non_null = 
  param_estimate %>% 
  filter(estimate != 0)

non_null %>% 
  group_by(sim_time) %>% 
  summarise(
  n_param = n()
) %>% 
  arrange(-n_param) %>%
  pull(n_param)

sum = 
  non_null %>%
  group_by(sim_time) %>%
  summarise(n_param = n()) %>% 
  arrange(-n_param)

max_sims = head(sum, 10) %>% pull(sim_time)
min_sims = tail(sum, 10) %>% pull(sim_time)
middle_sims = head(sum, 55) %>% tail(10) %>% pull(sim_time)

most = pmap(list(min_sims,what_number,what_method), get_MSE) %>% unlist()
least = pmap(list(max_sims,what_number,what_method), get_MSE) %>% unlist()
middle = pmap(list(middle_sims,what_number,what_method), get_MSE) %>% unlist()

result = 
  tibble(most, middle,least) %>% 
  pivot_longer(
    most:least,
    names_to = "missing",
    values_to = "RMSE"
    ) 

result = result %>% 
  add_column(n = c(rep(what_number, 30)), method = c(rep(what_method, 30)), ratio = c(rep("1:4:5", 30)))
result
}

forward_100_145 = getresulttable_145(what_number = 100, what_method = "forward")
forward_500_145 = getresulttable_145(what_number = 500, what_method = "forward")
forward_2000_145 = getresulttable_145(what_number = 2000, what_method = "forward")
lasso_100_145 = getresulttable_145(what_number = 100, what_method = "lasso_lmfit")
lasso_500_145 = getresulttable_145(what_number = 500, what_method = "lasso_lmfit")
lasso_2000_145 = getresulttable_145(what_number = 2000, what_method = "lasso_lmfit")

# generate RMSE for ratio 334
beta = c(rep(20, 12), rep(0.5, 28), rep(0, 60))

get_MSE <- function(sim_number,what_number,what_method){
  estimate = 
    read_csv("parameter_estimate2.csv",show_col_types = FALSE) %>%
    filter(n == what_number & method == what_method &sim_time == sim_number) %>% 
    mutate(type = factor(type, levels = c('strong', 'wbc', 'wai', 'null'))) %>% 
    arrange(type) %>% 
    pull(estimate)
  
  RMSE = sqrt(sum((estimate - beta) ^2) / 100)
  RMSE
}

getresulttable_334 <- function(what_number,what_method){
  param_estimate = 
    read_csv("parameter_estimate2.csv",show_col_types = FALSE) %>%
    filter(n == what_number & method == what_method)

non_null = 
  param_estimate %>% 
  filter(estimate != 0)

non_null %>% 
  group_by(sim_time) %>% 
  summarise(
  n_param = n()
) %>% 
  arrange(-n_param) %>%
  pull(n_param)

sum = 
  non_null %>%
  group_by(sim_time) %>%
  summarise(n_param = n()) %>% 
  arrange(-n_param)

max_sims = head(sum, 10) %>% pull(sim_time)
min_sims = tail(sum, 10) %>% pull(sim_time)
middle_sims = head(sum, 55) %>% tail(10) %>% pull(sim_time)

most = pmap(list(min_sims,what_number,what_method), get_MSE) %>% unlist()
least = pmap(list(max_sims,what_number,what_method), get_MSE) %>% unlist()
middle = pmap(list(middle_sims,what_number,what_method), get_MSE) %>% unlist()

result = 
  tibble(most, middle,least) %>% 
  pivot_longer(
    most:least,
    names_to = "missing",
    values_to = "RMSE"
    ) 

result = result %>% 
  add_column(n = c(rep(what_number, 30)), method = c(rep(what_method, 30)), ratio = c(rep("3:3:4", 30)))
result
}

forward_100_334 = getresulttable_334(what_number = 100, what_method = "forward")
forward_500_334 = getresulttable_334(what_number = 500, what_method = "forward")
forward_2000_334 = getresulttable_334(what_number = 2000, what_method = "forward")
lasso_100_334 = getresulttable_334(what_number = 100, what_method = "lasso_lmfit")
lasso_500_334 = getresulttable_334(what_number = 500, what_method = "lasso_lmfit")
lasso_2000_334 = getresulttable_334(what_number = 2000, what_method = "lasso_lmfit")

result = 
  forward_100_145 %>% 
  add_row(forward_500_145) %>% 
  add_row(forward_2000_145) %>% 
  add_row(lasso_100_145) %>% 
  add_row(lasso_500_145) %>% 
  add_row(lasso_2000_145) %>% 
  add_row(forward_100_334) %>% 
  add_row(forward_500_334) %>% 
  add_row(forward_2000_334) %>% 
  add_row(lasso_100_334) %>% 
  add_row(lasso_500_334) %>% 
  add_row(lasso_2000_334)

m1 = ggplot(filter(result, n == 100), aes(x = missing, y = RMSE, color = method)) + 
  geom_boxplot() +
  ylim(0,5) +
  theme(legend.position = "right") +
  labs(title = "RMSE",x = "missing", y = "RMSE") +
  facet_grid(. ~ ratio)

m2 = ggplot(filter(result, n == 500), aes(x = missing, y = RMSE, color = method)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "RMSE",x = "missing", y = "RMSE") +
  facet_grid(. ~ ratio)
  
m3 = ggplot(filter(result, n == 2000), aes(x = missing, y = RMSE, color = method)) + 
  geom_boxplot() +
  theme(legend.position = "right") +
  labs(title = "RMSE",x = "missing", y = "RMSE") +
  facet_grid(. ~ ratio)

```

```{r, echo = FALSE, fig.cap = "Model Complexity", fig.height = 8,fig.width = 7,warning=FALSE}
g_1
```

```{r, echo = FALSE, fig.cap = "Overall Classification Performance", fig.height = 8,fig.width = 9,warning=FALSE}
(g1 + g2) / (g3 + g4)
```

```{r, echo = FALSE, fig.cap = "Classification Performance by Signals", fig.height = 8,fig.width = 9,warning=FALSE}
(g5 + g6) / (g7 + g8)
```

```{r, echo = FALSE, fig.cap = "Parameter Estimation Performance", fig.height = 8,fig.width = 9,warning=FALSE}
(g9 + g12) / (g10 + g13) / (g11 + g14)
```

```{r, echo = FALSE, fig.cap = "RMSE comparison when n = 100", fig.height = 8,fig.width = 9, warning=FALSE}
m1
```

```{r, echo = FALSE, fig.cap = "RMSE comparison when n = 500", fig.height = 8,fig.width = 9}
m2
```

```{r, echo = FALSE, fig.cap = "RMSE comparison when n = 2000", fig.height = 8,fig.width = 9}
m3
```