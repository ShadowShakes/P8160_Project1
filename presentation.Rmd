---
title: |
  | Variable Selection Methods Comparison
author: |
  | Hongjie Liu,  Jiajun Tao,  Shaohan Chen
date: "2023-02-27"
header-includes:
   - \usepackage{graphicx}
   - \usepackage{float}
   - \usepackage{subfigure}
output:
  beamer_presentation:
    colortheme: "default"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

```{r include=FALSE}
# In high-dimensional data analysis, that is, the number of predictors p is no less than the number of observations n, variable selection is a common practice to find an optimal model that balances between model fitness and model complexity.

# However, In modern applications with high-dimensional covariates, traditional variable selection methods often struggle with the presence of “weak” predictors, that is, the coefficient is small but non-zero.
```

- Variable selection methods help to optimize models in high-dimensional settings where we need to select predictors that balance fitness and complexity.
 
- The presence of weak predictors is a problem that plagues traditional variable selection methods.


## Statistical Methods to be Studied

```{r include=FALSE}
# Specifically, we are interested in these two variable selection methods, that is, the step-wise forward method based on AIC, and automated LASSO regression. I assume you all are quite familiar with these two methods, so let's move on to the next slides.
```

**Step-wise forward method**

- Starts with the empty model, and iteratively adds the variables that best improves the model fit. That is often done by sequentially adding predictors with the largest reduction in AIC. For linear models,
$$AIC = n\log\left(\sum_{i=1}^n (y_i - \widehat{y}_i)^2/n\right) + 2p.$$

**Automated LASSO regression**
  
- Estimates model parameters by optimizing a penalized loss function:
$$\min_\beta \frac{1}{2n} \sum_{i=1}^n (y_i - x_i \beta )^2 + \lambda \sum_{k=1}^p|\beta_k|.$$


## Objectives

```{r include=FALSE}
# We would like to investigate how well each of the two methods in identifying weak and strong predictors, and how missing “weak” predictors impacts the parameter estimations.
```

(1) Evaluate the effectiveness of both methods in identifying weak and strong predictors.

(2) Examine how the absence of "weak" predictors affects parameter estimations.

## Types of Signals

```{r include=FALSE}
# First, we give a definition of what strong and weak predictors are. Specifically, we divide all predictors into 4 groups: strong, weak-but-correlated, weak-and-independent, and null predictors. We use this threshold $c\sqrt{\log (p) / n}$ to distinguish strong and weak, where c is a pre-specified parameter. For weak-but-correlated predictors, they are correlated with at least one strong predictors, while weak-and-independent are not. So, we will have the following result:
```

- Strong signals
$$S_{strong}=\{j:|\beta_j|>c\sqrt{\log (p) / n}\ \mbox{for some } c>0,\  1\le j \le p\}$$
- Weak-but-correlated (WBC) signals
\begin{align*}
S_{WBC}=&\ \{j: 0<|\beta_j|\le c\sqrt{\log (p) / n} \text{ and } \mbox{corr}(X_j, X_j')\ne 0\\
&\mbox{for some } j'\in S_1,\  1\le j \le p\}
\end{align*}
- Weak-and-independent (WAI) signals
\begin{align*}
S_{WAI}=&\ \{j: 0<|\beta_j|\le c\sqrt{\log (p) / n} \text{ and } \mbox{corr}(X_j, X_j')= 0\\
&\mbox{for all } j'\in S_1, \ 1\le j \le p\}
\end{align*}

- Null signals: $S_{null}=\{j: \beta_j=0,\  1\le j \le p\}$

## Types of Signals

```{r include=FALSE}
# These four types of predictors form a partition of all predictors. We use these notations to denote the number of each type. Note that the number of true predictors should be less than n.
```

Thus, $p$ predictors can be partitioned as $$\{1,\cdots,p\}=S_{strong}\cup S_{WBC}\cup S_{WAI}\cup S_{null}.$$

- We assume that $|S_{strong}|=p_{S}$, $|S_{WBC}|=p_{WBC}$, $|S_{WAI}|=p_{WAI}$.

- The number of true predictors $p_{S}+p_{WBC}+p_{WAI}$ should be less than $n$.


## Data Generation

```{r include=FALSE}
# Now let's move on to the data generation section! We assume that normality assumption is met. And we set the coefficients of strong signals to be 20 and those of weak signals to be 0.5. Since c is pre-specified, we select it to be 20 so that this inequation are met for all scenarios to be investigated further. Also, we set sigma squared to be 8.
```

- Normality assumption $$\mathbf{y}\sim N(\mathbf{X}\mathbf{\beta},\sigma^2\mathbf{I})$$

- For $j\in S_{strong}$, $\beta_j=20$; For $j\in S_{WBC}\cup S_{WAI}$, $\beta_j=0.5$.

- We choose $c=20$ so that $0.5\leq c\sqrt{\log p/n} < 20$ for all scenarios to be investigated.

- $\sigma^2 = 8.$


## Data Generation - Random Number Generation (Binary Outcome)

```{r, eval=FALSE}
set.seed(20220217)
seed_vec <- runif(100000, min, max) 
  for (i in 1:n) {
    set.seed(seeds[i])
    long_rnorm <- rnorm(size*3, mean = 0, sd = 1)
    long_runif <- runif(size*2)
    beta_error <- rnorm(size, mean = 0, sd = 0.25)
    L1 <- long_rnorm[1:size]
    L2 <- long_rnorm[(size + 1):(2*size)]
    L3 <- long_rnorm[(2*size + 1):(3*size)]
    
    comp_pA = long_runif[1:size]
    A = (prob_A > comp_pA) 
    # function continues...
    }
```


## Parameters of Interest

- The sample size of each dataset $n_{\text{sample}} \in \{ 100, 1000 \}$
- The population proportion of treated individuals $\pi \in \{ 0.113, 0.216, 0.313 \}$
- The true average treatment effect $\beta_1 \in \{0.15, 0.30 \}$ for binary data; $\beta_1 \in \{-1, 1 \}$ for continuous data

_Other Parameters_

- The number of datasets $m_{\text{sample}} = 100$
- The number of bootstrap re-samples $m_{\text{boot}} = 500$
- The sample size of bootstrap re-samples $n_{\text{simple}} = n_{\text{complex}} = n_{\text{sample}}\times \pi$
- Strength of covariate effect on treatment $\alpha_1 = \log(1.25), \alpha_2 = \log(1.75)$
- Strength of covariate effect on outcome $\beta_2 = \log(1.75), \beta_3 = \log(1.25)$


## Evaluation Metrics

Define true predictors as positive and null predictors as negative\par

_Signal Identification_

- \textbf{Complexity:} $\text{Number of Selected Parameters}$

- \textbf{Sensitivity:} $\frac{TP}{TP+FN}$

- \textbf{Specificity:} $\frac{TN}{TN+FP}$

- \textbf{F1-Score:} $\frac{2\cdot \text{sensitivity}\cdot \text{precision}}{\text{sensitivity}+\text{precision}}$

- \textbf{Accuracy:} $\frac{TP+TN}{TP+TN+FP+FN}$

_Parameter Estimation_

- \textbf{MSE:} $\frac{1}{p}\sum_{i=1}^p (\hat{\beta_i}-\beta_i)^2$


## Signal Identification Performance - Complexity
\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/g_1.pdf} 
\caption{Model Complexity} 
\label{Fig_1}
\end{figure}


## Signal Identification Performance - Complexity Exploration
- When in high dimensional scenario, forward selection tends to select nearly all of the predictors but Lasso does not
- Lasso tends to select much fewer predictors than forward selection, and the parameters it select will increase as n increases
- As n increases, the predictors that two models select are more precise(closer to true predictor number 40)

## Signal Identification Performance - Sensitivity
\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/g1.pdf} 
\caption{Sensitivity Performance} 
\label{Fig1}
\end{figure}


## Signal Identification Performance - Sensitivity
\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/g5.pdf} 
\caption{Sensitivity of True Signals} 
\label{Fig5}
\end{figure}


## Signal Identification Performance - Sensitivity Exploration
- Forward selection are highly sensitive in high dimensional case(n=100) while Lasso does not
- Overall, the sensitivity of two models increases as n increases
- Both models are sensitive in selecting strong signals. But when it comes to weak predictors, Lasso is much less sensitive in high dimensional scenario than forward selection.
- When n increases, the sensitivity discrepancy of selecting weak predictors between two models becomes smaller. But still, forward selection is overall more sensitive than Lasso
- The sensitivity discrepancy between two models are smaller when the ratio of strong predictors becomes larger


## Signal Identification Performance - Specificity
\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/g2.pdf} 
\caption{Specificity Performance} 
\label{Fig2}
\end{figure}


## Signal Identification Performance - Specificity Exploration
- In high dimensional scenario, the specificity of forward selection is near 0, which means it almost does not identify any null predictor, but Lasso in turn has high specificity
- In high dimensional scenario, forward selection is very assertive and tends to identity all 100 predictors as true, leading to extremely high sensitivity but low specificity
- In high dimensional scenario, Lasso is very conservative and tends to identity most 100 predictors as null,leading to low sensitivity but high specificity
- As n increases, forward selection has higher specificity. And overall, specificity are higher when the ratio of strong predictors are lower


## Signal Identification Performance - F1-Score
\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/g3.pdf} 
\caption{F1-Score Performance} 
\label{Fig3}
\end{figure}


## Signal Identification Performance - F1-Score

\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/g7.pdf} 
\caption{F1-Score of True Signals} 
\label{Fig7}
\end{figure}


## Signal Identification Performance - F1-score Exploration
- Lasso has lower F1-Score when n is not large. When n=2000, both models have similarly high F1-score
- F1-score will increase significantly for both models, when the ratio of strong predictors are larger. And F1-score is also higher when the ratio of strong predictors are larger
- Strong predictors have F1-Score=1 for each scenario, and weak predictors have higher F1-score when n increases for Lasso


## Signal Identification Performance - Accuracy
\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/g4.pdf} 
\caption{Accuracy Performance} 
\label{Fig4}
\end{figure}


## Signal Identification Performance - Accuracy
\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/g8.pdf} 
\caption{Accuracy of Different Signals} 
\label{Fig8}
\end{figure}

## Signal Identification Performance - Accuracy Exploration
- Accuracy is low in high dimensional scenario, especially forward selection
- Accuracy increases for both models when n increase 
- Accuracy is higher when the ratio of strong predictors is higher, and Lasso has overall higher accuracy than forward
- In high dimensional, forward has higher accuracy for weak predictors than Lasso


## Parameter Estimation Performance - n=100
\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/g9.pdf} 
\caption{Beta Rmse when n=100} 
\label{Fig9}
\end{figure}


## Parameter Estimation Performance - n=500
\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/g10.pdf} 
\caption{Beta Rmse when n=500} 
\label{Fig10}
\end{figure}


## Parameter Estimation Performance - n=2000
\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/g11.pdf} 
\caption{Beta Rmse when n=2000} 
\label{Fig11}
\end{figure}


## Parameter Estimation Performance - Exploration
- In high-dimensional scenario, Lasso performs much better than forward
- Lasso performs better than forward when n=500, but worse than forward when n=2000


## Predictors Identification Conclusions

_High Dimensional Scenario(n=100)_

- Forward selection tend to be assertive, with extremely high sensitivity and more selected predictors, better at identifying true signals
- Lasso tend to be conservative, with extremely high specificity and fewer selected predictors, better at identifying null signals
- Forward selection has an overall much higher F1-score but lower accuracy than Lasso
- Lasso performs smaller RMSE on parameter estimation than forward selection


## Predictors Identification Conclusions

_Normal Scenario(n=500, 2000)_

- Forward selection tends to overall select more predictors than Lasso, both methods select more predictors and more close to true predictor number(40) as n increases
- Both methods are sensitive for strong predictors. Forward selection is more sensitive than Lasso in identifying weak predictors than Lasso when there are fewer true strong predictors, else the discrepancy becomes smaller
- The sensitivity for identifying weak predictors are increasing as n increases for both models

## Predictors Identification Conclusions

_Normal Scenario(n=500, 2000)_

- Lasso overall has higher specificity than forward, but the discrepancy becomes smaller as n increases
- The F1-score and accuracy increases as n increases for both models, overall Lasso has higher accuracy than forward 
- Lasso performs smaller RMSE on parameter estimation than forward as n increases

## Missing Weak Predictors Analysis - Introduction

How missing "weak" predictors impacts the parameter estimations

Definition: missing weak predictors = true weak predictors but estimated as null

How to value parameter estimations: RMSE

## Missing Weak Predictors Analysis - Methods

How to value parameter estimations: RMSE

Most missing: simulations that have the least non-null estimations

Least missing: simulations that have the most non-null estimations

Middle: in between

## Missing Weak Predictors Analysis - Result: n=100

\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/m_1.pdf} 
\caption{RMSE when n=100} 
\label{Fig12}
\end{figure}

## Missing Weak Predictors Analysis - Result: n=500

\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/m_2.pdf} 
\caption{RMSE when n=500} 
\label{Fig13}
\end{figure}

## Missing Weak Predictors Analysis - Result: n=2000

\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/m_3.pdf} 
\caption{RMSE when n=2000} 
\label{Fig14}
\end{figure}

## Missing Weak Predictors Analysis - Discussion

- No apparent patterns between different ratios

- In high-dimensional scenarios, Lasso performs much better than forward selection according to RMSE, no matter how much missing.

- When n is large enough, RMSE of both methods become small.

- When n = 500, Lasso is slightly better than forward selection, however, when n = 2000, just the reverse.

- In Lasso, RMSE seems to increase if the missing amount increases, but in forward selection, RMSE decreases when missing amount increases.


## Summary of Results

- For binary outcomes, the simple bootstrap tended to underestimate the standard error

- Larger standard error estimates from complex bootstrap in binary and continuous settings

- Differences between simple and complex bootstrap were smaller for larger sample sizes

- Complex bootstrap not as reliable in small sample sizes


## Limitations

- Sample size / treatment (or exposure) prevalence

- Small number of initial samples, limited in detecting significant differences in coverage rate


## Future Work

- Larger number of initial samples, narrower coverage window

- Increased sample size, changes in bootstrap performance?

- Changes in treatment propensity model

- Non-normal distributions of covariates