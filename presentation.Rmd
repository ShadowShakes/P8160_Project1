---
title: |
  | Variable Selection Methods Comparison
author: |
  | Hongjie Liu,  Jiajun Tao,  Shaohan Chen
date: "2023-02-27"
header-includes:
   - \usepackage{graphicx}
   - \usepackage{float}
   - \usepackage{subfigure}
output:
  beamer_presentation:
    colortheme: "default"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Outline

- Background

- Statistical methods to be studied

- Objective

- Simulation

- Experiment Settings and Scenario

- Model Evaluation

- Missing Weak Predictor Analysis

- Discussions and conclusions

- Limitations and Future Work

- Reference

- Q&A

## Background

```{r include=FALSE}
# In linear models, when the number of predictors p is large, or even the data is high-dimensional, that is, p is no less than the number of observations n, variable selection is a common practice to find an optimal model that balances between model fitness and model complexity.

# However, in modern applications with high-dimensional covariates, traditional variable selection methods often struggle with the presence of “weak” predictors, that is, the coefficient is small but non-zero.
```

- Variable selection methods help to optimize models in high-dimensional settings where we need to select predictors that balance fitness and complexity.

- The presence of weak predictors is a problem that plagues traditional variable selection methods.


## Statistical Methods to be Studied

```{r include=FALSE}
# Specifically, we are interested in these two variable selection methods, the step-wise forward method based on AIC criteria, and the automated LASSO regression. I assume you all are quite familiar with these two methods, so let's move on to the next slides.
```

**Step-wise forward method**

- Starts with the empty model, and iteratively adds the variables that best improve the model fit. That is often done by sequentially adding predictors with the largest reduction in AIC. For linear models,
$$AIC = n\log\left(\sum_{i=1}^n (y_i - \widehat{y}_i)^2/n\right) + 2p.$$

**Automated LASSO regression**
  
- Estimates model parameters by optimizing a penalized loss function:
$$\min_{\boldsymbol{\beta}} \frac{1}{2n} \sum_{i=1}^n (y_i - \boldsymbol{x}_i \boldsymbol{\beta} )^2 + \lambda \sum_{k=1}^p|\beta_k|.$$


## Objectives

```{r include=FALSE}
# We would like to investigate how well each of the two methods is in identifying weak and strong predictors, and how missing “weak” predictors impacts the parameter estimations.
```

(1) Evaluate the effectiveness of both methods in identifying weak and strong predictors.

(2) Examine how the absence of "weak" predictors affects parameter estimations.


## Types of Signals

```{r include=FALSE}
# First, we give a definition of what strong and weak predictors are. Specifically, we divide all predictors into 4 groups: strong, weak-but-correlated, weak-and-independent, and null predictors. We use this threshold $c\sqrt{\log (p) / n}$ to distinguish strong and weak, where c is a pre-specified parameter. For weak-but-correlated predictors, they are correlated with at least one strong predictor, while weak-and-independent are not. So, we will have the following result:
```

- Strong signals
$$S_{strong}=\{j:|\beta_j|>c\sqrt{\log (p) / n}\ \mbox{for some } c>0,\  1\le j \le p\}$$
- Weak-but-correlated (WBC) signals
\begin{align*}
S_{WBC}=&\ \{j: 0<|\beta_j|\le c\sqrt{\log (p) / n} \text{ and } \mbox{corr}(X_j, X_j')\ne 0\\
&\mbox{for some } j'\in S_1,\  1\le j \le p\}
\end{align*}
- Weak-and-independent (WAI) signals
\begin{align*}
S_{WAI}=&\ \{j: 0<|\beta_j|\le c\sqrt{\log (p) / n} \text{ and } \mbox{corr}(X_j, X_j')= 0\\
&\mbox{for all } j'\in S_1, \ 1\le j \le p\}
\end{align*}

- Null signals: $S_{null}=\{j: \beta_j=0,\  1\le j \le p\}$

## Types of Signals

```{r include=FALSE}
# These four types of predictors form a partition of all predictors. We use these notations to denote the number of each type. Note that the number of true predictors should be less than n.
```

Thus, $p$ predictors can be partitioned as $$\{1,\cdots,p\}=S_{strong}\cup S_{WBC}\cup S_{WAI}\cup S_{null}.$$

- We assume that $|S_{strong}|=p_{S}$, $|S_{WBC}|=p_{WBC}$, $|S_{WAI}|=p_{WAI}$.

- The number of true predictors $p_{S}+p_{WBC}+p_{WAI}$ should be less than $n$.


## Data Generation

```{r include=FALSE}
# Now let's move on to the data generation section! We assume that normality is met. And we set the coefficients of strong signals to 20 and those of weak signals to 0.5. Since c is pre-specified, we select it to be 20 so that this inequation is met for all scenarios to be investigated further. Also, we set sigma to 8.
```

- Normality assumption $$\mathbf{y}\sim N(\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})$$

- For $j\in S_{strong}$, $\beta_j=20$; For $j\in S_{WBC}\cup S_{WAI}$, $\beta_j=0.5$.

- We choose $c=20$ so that $0.5\leq c\sqrt{\log p/n} < 20$ for all scenarios to be investigated.

- For the error term, we set $\sigma = 8.$


## Data Generation - Design Matrix

```{r include=FALSE}
# How do we generate the design matrix?
# We assume that X follows a joint normal distribution.
# We standardize all predictors, then the covariance matrix sigma here is the correlation matrix, thus the mean is zero and all diagonal elements of covariance matrix are 1.
# Remember weak-but-correlated predictors have correlation with strong predictors? We set they are correlated with one and only one strong predictor.
# We use the mvrnorm function from the package MASS to generate multivariate normal distribution data.
```

- We assume $$\mathbf{X}\sim N(\boldsymbol{\mu},\boldsymbol{\Sigma})$$

- All predictors are standardized. Then we have $\boldsymbol{\mu}=\mathbf{0}$ and $\Sigma_{i,i}=1$ for all $i$.

- We set $p_{WBC}\geq p_{strong}$. For each strong predictor (except one of them), we set $[p_{WBC}/p_{strong}]$ WBC predictors to be correlated with it. Each WBC predictor is set to be correlated with one and only one strong predictor.

- All other elements of $\boldsymbol{\Sigma}$ are 0.

We use the `MASS::mvrnorm` function to generate data following a multivariate normal distribution.


## Data Generation - Simulation Code

```{r include=FALSE}
# This is the R code for generating the data.
```

```{r eval=FALSE}
corr_matrix = matrix(rep(0, len = p^2), nrow = p)
corr_num = pwbc %/% ps
for (i in 1:(ps - 1)) {
  for (j in (ps + 1 + (i - 1)*corr_num):(ps + i*corr_num)) {
    corr_matrix[i, j] = corr
    corr_matrix[j, i] = corr
  }
}
for (j in (ps + 1 + (ps - 1)*corr_num):(ps + pwbc)) {
    corr_matrix[ps, j] = corr
    corr_matrix[j, ps] = corr
}
diag(corr_matrix) = 1
X = MASS::mvrnorm(n, mu = rep(0, p), Sigma = corr_matrix)
beta = c(rep(20, ps), rep(0.5, pwbc + pwai),
         rep(0, p - ps - pwbc - pwai))
Y = X %*% beta + rnorm(n, mean = 0, sd = 8)
```


## Investigation Settings and Scenarios

_Fixed_

- Number of parameters: $p = 100$

- Ratio of true and null signals: $2:3$

- Correlation between strong and WBC: $corr = 0.4$

_Unfixed_

- Number of observations: $n = 100\text{ (high dimensional)}, 500, 2000$

- Ratio of strong, WBC, and WAI signals: $p_{strong }:p_{WBC}:p_{WAI}=1:4:5 \ \text{and} \ 3:3:4$


## Evaluation Metrics

Define true predictors as positive and null predictors as negative\par

_Signal Identification_

- \textbf{Complexity:} $\text{Number of Selected Parameters}$

- \textbf{Sensitivity:} $\frac{TP}{TP+FN}$

- \textbf{Specificity:} $\frac{TN}{TN+FP}$

- \textbf{F1-score:} $\frac{2\cdot \text{sensitivity}\cdot \text{precision}}{\text{sensitivity}+\text{precision}}$

- \textbf{Accuracy:} $\frac{TP+TN}{TP+TN+FP+FN}$

_Parameter Estimation_

- \textbf{RMSE:} $\sqrt{\frac{1}{p}\sum_{i=1}^p (\hat{\beta_i}-\beta_i)^2}$

- \textbf{Variance:} $\sqrt{\frac{1}{p}\sum_{i=1}^p (\hat{\beta_i}-\bar{\beta_i})^2}$


## Signal Identification Performance - I
\begin{figure}[H] 
\includegraphics[width=0.8\textwidth]{Images/g_1.pdf} 
\end{figure}


## Signal Identification Performance - II
\begin{figure} 
  \centering 
  \subfigure[Sensitivity]{ 
    \label{sub1}
    \includegraphics[width=2.0in, height = 1.4in]{Images/g1.pdf} 
  } 
  \subfigure[Specificity]{ 
    \label{sub2} 
    \includegraphics[width=2.0in, height = 1.4in]{Images/g2.pdf} 
  } 
  \subfigure[F1-Score]{ 
    \label{sub2} 
    \includegraphics[width=2.0in, height = 1.4in]{Images/g3.pdf} 
  } 
  \subfigure[Accuracy]{ 
    \label{sub2} 
    \includegraphics[width=2.0in, height = 1.4in]{Images/g4.pdf} 
  } 
  \label{para1} 
\end{figure}


## Signal Identification Performance - II
\begin{figure} 
  \centering 
  \subfigure[Sensitivity by Signal]{ 
    \label{sub1}
    \includegraphics[width=2.0in, height = 1.4in]{Images/g5.pdf} 
  } 
  \subfigure[F1-Score by Signal]{ 
    \label{sub2} 
    \includegraphics[width=2.0in, height = 1.4in]{Images/g7.pdf} 
  } 
  \subfigure[Accuracy by Signal]{ 
    \label{sub2} 
    \includegraphics[width=2.0in, height = 1.4in]{Images/g8.pdf} 
  } 
  \caption{fig} 
  \label{para1} 
\end{figure}


## Signal Identification Performance - III
\begin{figure} 
  \centering 
  \subfigure[RMSE n=100]{ 
    \label{sub1}
    \includegraphics[width=1.8in,height = 0.8in]{Images/g9.pdf} 
  } 
  \subfigure[Variance n=100]{ 
    \label{sub2} 
    \includegraphics[width=1.8in,height = 0.8in]{Images/g12.pdf} 
  } 
  \subfigure[RMSE n=500]{ 
    \label{sub3}
    \includegraphics[width=1.8in,height = 0.8in]{Images/g10.pdf} 
  } 
  \subfigure[Variance n=500]{ 
    \label{sub4} 
    \includegraphics[width=1.8in,height = 0.8in]{Images/g13.pdf} 
  }
  \subfigure[RMSE n=2000]{ 
    \label{sub5}
    \includegraphics[width=1.8in,height = 0.8in]{Images/g11.pdf} 
  } 
  \subfigure[Variance n=2000]{ 
    \label{sub6} 
    \includegraphics[width=1.8in,height = 0.8in]{Images/g14.pdf} 
  } 
  \label{para1} 
\end{figure}


## Predictors Identification Conclusions

_High Dimensional Scenario(n=100)_

- Forward selection tends to select lots of predictors and Lasso tends to select few

- Forward selection tends to be assertive and better at identifying weak signals, with extremely high sensitivity, low specificity

- Lasso tend to be conservative and better at identifying null signals, with extremely high specificity, low sensitivity

- Both identify strong predictors perfectly


## Predictors Identification Conclusions

_High Dimensional Scenario(n=100)_

- Forward selection performs better than Lasso on F1-score but worse on overall accuracy

- Both models seem to be too radical in predictor identification

- Lasso performs better on parameter estimation than forward selection


## Predictors Identification Conclusions

_Normal Scenario(n=500, 2000)_

- Forward selection tends to select more predictors than Lasso, but both get closer to actual number as n increases

- Lasso is better at identifying null predictors than forward selection, but poorer at identifying other weak predictors (under-screening). Both models nicely identify strong predictors. 

- Identification differences of all metrics are narrowed down with n increasing


## Predictors Identification Conclusions

_Normal Scenario(n=500, 2000)_

- When there are more strong predictors, weak-but-correlated predictors become easier to be identified, especially for Lasso. However, parameter estimation also becomes more unstable

- Lasso performs better on parameter estimation than forward when n is not large, and conversely as n increases


## Missing Weak Predictors Analysis - Introduction

How missing "weak" predictors impacts the parameter estimations

Definition: missing weak predictors = true weak predictors but estimated as null

How to value parameter estimations: RMSE

## Missing Weak Predictors Analysis - Methods

How to value parameter estimations: RMSE

Most missing: simulations that have the least non-null estimations

Least missing: simulations that have the most non-null estimations

Middle: in between

## Missing Weak Predictors Analysis - Result: n=100

\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/m_1.pdf} 
\caption{RMSE when n=100} 
\label{Fig12}
\end{figure}

## Missing Weak Predictors Analysis - Result: n=500

\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/m_2.pdf} 
\caption{RMSE when n=500} 
\label{Fig13}
\end{figure}

## Missing Weak Predictors Analysis - Result: n=2000

\begin{figure}[H] 
\includegraphics[width=0.9\textwidth]{Images/m_3.pdf} 
\caption{RMSE when n=2000} 
\label{Fig14}
\end{figure}

## Missing Weak Predictors Analysis - Discussion

- No apparent patterns between different ratios

- In high-dimensional scenarios, Lasso performs much better than forward selection according to RMSE, no matter how much missing.

- When n is large enough, RMSE of both methods become small.

- When n = 500, Lasso is slightly better than forward selection, however, when n = 2000, just the reverse.

- In Lasso, RMSE seems to increase if the missing amount increases, but in forward selection, RMSE decreases when missing amount increases.


## Limitations

- Correlation matrix Pattern

- Number of observations $n$ and number of parameters $p$

- Ratio of Strong, Wbc, and Wai predictors


## Future Work

- Larger number of initial samples, narrower coverage window

- Increased sample size, changes in bootstrap performance?

- Changes in treatment propensity model

- Non-normal distributions of covariates